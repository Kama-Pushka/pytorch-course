# Домашнее задание к уроку 3: Полносвязные сети

Примерное время выполнения: 5 часов.

## Цель задания
Изучить влияние архитектуры полносвязных сетей на качество классификации, провести эксперименты с различными конфигурациями моделей.

## Задание 1: Эксперименты с глубиной сети (30 баллов)

Создайте файл `homework_depth_experiments.py`:

### 1.1 Сравнение моделей разной глубины (15 баллов)
Создайте и обучите модели с различным количеством слоев:
- [x] 1 слой (линейный классификатор)
- [x] 2 слоя (1 скрытый)
- [x] 3 слоя (2 скрытых)
- [x] 5 слоев (4 скрытых)
- [x] 7 слоев (6 скрытых)

```
# Для каждого варианта:
# - Сравните точность на train и test
# - Визуализируйте кривые обучения
# - Проанализируйте время обучения
```

---

В качестве датасетов использовались датасеты MNIST и CIFAR10.
Для 2 и 3 заданий использовался датасет MNIST.

Превью скомпилированных графиков для MNIST ([фулл](plots%2F1.1MNIST_general.png)). Также в `plots/` лежат графики для CIFAR10.

![1.1MNIST_general.png](plots%2F1.1MNIST_general.png)

В целом можно заметить, что для тестовых данных метрики имеют лучшие показатели по сравнению с обучающими. При увеличении количества слоев время обучения возрастает (подробнее [depth_experiments.txt](results%2Fdepth_experiments.txt)) 

### 1.2 Анализ переобучения (15 баллов)
```python
# Исследуйте влияние глубины на переобучение:
# - Постройте графики train/test accuracy по эпохам
# - Определите оптимальную глубину для каждого датасета
# - Добавьте Dropout и BatchNorm, сравните результаты
# - Проанализируйте, когда начинается переобучение
```

---

Взглянув на график выше, можно заметить, что для данных датасетов после добавления 3 слоев заметного роста модели не наблюдается. При добавлении Dropout и BatchNorm слоев (для 5 слойной модели) заметно некоторое улучшение метрик, особенно на первых эпохах. 
![1.1MNIST_5 layers.png](plots%2F1.1MNIST_5%20layers.png)
![1.1MNIST_Dropout + BatchNorm layers.png](plots%2F1.1MNIST_Dropout%20%2B%20BatchNorm%20layers.png)

На данных материалах не наблюдается переобучения моделей, однако одними из причин начала переобучения для представленных моделей могут быть **отсутствие регуляризации**, т.к. по мере обучения модель будет стремиться более точно подогнаться к обучающим данным, теряя тем самым свойство обобщаемости. 
Другой причиной переобучения может стать **неверная настройка гиперпараметров**, например слишком большое значение эпох или чрезмерная глубина архитектуры. В совокупности эти два признака самостоятельно могут привести к нестабильности обучения и усиленному переобучению.


## Задание 2: Эксперименты с шириной сети (25 баллов)

Создайте файл `homework_width_experiments.py`:

### 2.1 Сравнение моделей разной ширины (15 баллов)

Создайте модели с различной шириной слоев:
- [x] Узкие слои: [64, 32, 16]
- [x] Средние слои: [256, 128, 64]
- [x] Широкие слои: [1024, 512, 256]
- [x] Очень широкие слои: [2048, 1024, 512]

```
# Для каждого варианта:
# - Поддерживайте одинаковую глубину (3 слоя)
# - Сравните точность и время обучения
# - Проанализируйте количество параметров
```

---

[width_experiments.txt](results%2Fwidth_experiments.txt)

Все модели показали высокую точность, более широкие слои обучались дольше меньших, однако на моем устройстве и данном датасете эта разница оказалась невеликой. При увеличении ширины слоев соответственно возрастает и количество параметров модели. 

### 2.2 Оптимизация архитектуры (10 баллов)
```python
# Найдите оптимальную архитектуру:
# - Используйте grid search для поиска лучшей комбинации
# - Попробуйте различные схемы изменения ширины (расширение, сужение, постоянная)
# - Визуализируйте результаты в виде heatmap
```

---

Воспользовавшись grid search удалось получить лучшую комбинацию из ряда возможных параметров ширины. Лучшей оказалась модель (1024, 1024, 1024), точность которой составила 0.9755, а количество параметров равно 2,913,400.

![2.2.png](plots%2F2.2.png)

## Задание 3: Эксперименты с регуляризацией (25 баллов)

Создайте файл `homework_regularization_experiments.py`:

### 3.1 Сравнение техник регуляризации (15 баллов)
Исследуйте различные техники регуляризации:
- [x] Без регуляризации
- [x] Только Dropout (разные коэффициенты: 0.1, 0.3, 0.5)
- [x] Только BatchNorm
- [x] Dropout + BatchNorm
- [x] L2 регуляризация (weight decay)

```
# Для каждого варианта:
# - Используйте одинаковую архитектуру
# - Сравните финальную точность
# - Проанализируйте стабильность обучения
# - Визуализируйте распределение весов
```

---

[regularization_experiments.txt](results%2Fregularization_experiments.txt)

В целом, все модели продемонстрировали высокий, почти не отличающийся друг от друга результат, что говорит о том, что модель для данного датасета крайне быстро приходит к высоким метрикам без дополнительной регуляризации. Соответственно и стабильностью модели обладают высокой.

![3.1.png](plots%2F3.1.png)

### 3.2 Адаптивная регуляризация (10 баллов)

Реализуйте адаптивные техники:
- [ ] Dropout с изменяющимся коэффициентом
- [ ] BatchNorm с различными momentum
- [ ] Комбинирование нескольких техник
- [ ] Анализ влияния на разные слои сети

---

При реализации данной задачи возникли проблемы в понимании и реализации данных адаптивных техник. Шото сделано не так, и модель начинает быстро деградировать.