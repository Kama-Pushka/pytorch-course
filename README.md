# Домашнее задание к уроку 4: Сверточные сети

Примерное время выполнения: >6 часов

## Цель задания
Сравнить эффективность сверточных и полносвязных сетей на задачах компьютерного зрения, изучить преимущества CNN архитектур.

## Задание 1: Сравнение CNN и полносвязных сетей (40 баллов)

Создайте файл `homework_cnn_vs_fc_comparison.py`:

### 1.1 Сравнение на MNIST (20 баллов)
Сравните производительность на MNIST:
- [x] Полносвязная сеть (3-4 слоя)
- [x] Простая CNN (2-3 conv слоя)
- [x] CNN с Residual Block
``` 
# Для каждого варианта:
# - Обучите модель с одинаковыми гиперпараметрами
# - Сравните точность на train и test множествах
# - Измерьте время обучения и инференса
# - Визуализируйте кривые обучения
# - Проанализируйте количество параметров
```

---
[mnist_comparison.txt](results%2Fmnist_comparison.txt)

Из результатов обучения видно, что точность на train и test множествах для всех трех моделях практически совпадает.

```commandline
Параметры:
- FCN: 567,434
- Simple CNN: 421,642
- CNN with Residual: 160,906

Время обучения:
- FCN: 75.76 сек.
- Simple CNN: 88.00 сек.
- CNN with Residual: 88.12 сек.

Среднее время инференса на одну партию:
- FCN: 0.0002 сек.
- Simple CNN: 0.0003 сек.
- CNN with Residual: 0.0005 сек.
```

![1.1mnist.png](plots%2F1.1mnist.png)

В целом полносвязная модель на данном датасете показала себя немного хуже, чем две модели, основанные на сверточных слоях. 
Время обучения и инференса всех трех моделей примерно сопоставимы, однако для сверточных сетей эти два процесса занимают немного больше времени. 
Также заметно значительное снижение кол-ва параметров у сверточных сетей, по сравнению с полносвязной.

### 1.2 Сравнение на CIFAR-10 (20 баллов)

Сравните производительность на CIFAR-10:
- [x] Полносвязная сеть (глубокая)
- [x] CNN с Residual блоками
- [x] CNN с регуляризацией и Residual блоками

```
# Для каждого варианта:
# - Обучите модель с одинаковыми гиперпараметрами
# - Сравните точность и время обучения
# - Проанализируйте переобучение
# - Визуализируйте confusion matrix
# - Исследуйте градиенты (gradient flow)
```

---

[cifar_comparison.txt](results%2Fcifar_comparison.txt)

```commandline
Параметры:
- FCN: 9,089,720
- CNN with Residual: 2,785,354
- CNN with Reg + Residual: 2,785,354

Время обучения:
- FCN: 127.61 сек.
- CNN with Residual: 287.91 сек.
- CNN with Reg + Residual: 298.26 сек.
```

![1.2cifar.png](plots%2F1.2cifar.png)
![1.2cifar_confusion matrix.png](plots%2F1.2cifar_confusion%20matrix.png)

Из результатов обучения можно заметить, что текущая реализация полносвязной сети не способна обучатся на CIFAR-датасете.
В то же время сверточные модели с Residual слоями и регуляризацией хорошо справились с задачей классификации картинок.
Модель с регуляризацией показала более стабильное обучение, чем модель без.

Явное переобучение для сверточных моделей при текущих гиперпаматерах не наблюдается, однако заметно явное снижение темпов обучения после 5 эпохи для обеих моделей, что может сигнализировать о начале переобучения.

![1.2CNN.png](plots%2F1.2CNN.png)
![1.2CNN with Reg.png](plots%2F1.2CNN%20with%20Reg.png)

Из графиков значений градиентов на слоях можно увидеть, что значения градиентов для модели с регуляризацией слегка ниже, чем для модели без неё.
В обеих моделях наблюдаются пики вначале (нормальное явление) и в последнем слое, что может указывать как на эффект накопления градиентов, так и на особенности архитектуры. 

## Задание 2: Анализ архитектур CNN (30 баллов)

Создайте файл `homework_cnn_architecture_analysis.py`:

### 2.1 Влияние размера ядра свертки (15 баллов)

Исследуйте влияние размера ядра свертки:
- [x] 3x3 ядра
- [x] 5x5 ядра
- [x] 7x7 ядра
- [x] Комбинация разных размеров (1x1 + 3x3)

```
# Для каждого варианта:
# - Поддерживайте одинаковое количество параметров
# - Сравните точность и время обучения
# - Проанализируйте рецептивные поля
# - Визуализируйте активации первого слоя
```

---

[architecture_analysis.txt](results%2Farchitecture_analysis.txt)

```commandline
Summary Table:
| Model Name               |   Training Time (sec) | Final Test Accuracy (Average)  |
|--------------------------+-----------------------+--------------------------------|
| 3x3 Kernel               |                111.44 | 0.73323                        |
| 5x5 Kernel               |                115.83 | 0.71573                        |
| 7x7 Kernel               |                116.38 | 0.63519                        |
| Mixed Kernel (1x1 + 3x3) |                115.14 | 0.65761                        |

Receptive Fields:
3x3 Kernel: 7x7
5x5 Kernel: 13x13
7x7 Kernel: 19x19
Mixed Kernel (1x1 + 3x3): 7x7
```

Наибольшую точность на тестовой выборке среди 4 моделей показала модель с ядром 3x3, по мере увеличения ядра точность моделей падала.
По результатам обучения видно, что после 5-6 эпохи все модели начали переобучаться (train loss снижался, test loss увеличивался) или обучение происходило нестабильно.
По мере увеличения ядра увеличивается и размер рецептивного поля для последующих слоев.

Активации первого слоя для 5x5 и 3x3 + 1x1 соответственно ([остальные](plots)):
![2.1_5x5.png](plots%2F2.1_5x5.png)
![2.1_1x1 + 3x3.png](plots%2F2.1_1x1%20%2B%203x3.png)

Заметно, что ядро 3x3 + 1x1 гораздо лучше выделяет примитивные признаки, чем остальные ядра.


### 2.2 Влияние глубины CNN (15 баллов)

Исследуйте влияние глубины CNN:
- [x] Неглубокая CNN (2 conv слоя)
- [x] Средняя CNN (4 conv слоя)
- [x] Глубокая CNN (6+ conv слоев)
- [x] CNN с Residual связями

```
# Для каждого варианта:
# - Сравните точность и время обучения
# - Проанализируйте vanishing/exploding gradients
# - Исследуйте эффективность Residual связей
# - Визуализируйте feature maps
```

---

```commandline
Summary Table:
+-----------------------------+-------------------------+----------------------------------------------------------------------------------+
|            Model            | Training Time (seconds) |                               Final Test Accuracy                                |
+-----------------------------+-------------------------+----------------------------------------------------------------------------------+
|            Model            |   Training Time (sec)   |                               Final Test Accuracy                                |
| Shallow CNN (2 conv layers) |         112.59          |  [0.6176, 0.679, 0.7085, 0.7231, 0.7287, 0.7272, 0.729, 0.7324, 0.7281, 0.726]   |
| Medium CNN (4 conv layers)  |         114.92          | [0.5886, 0.6885, 0.7141, 0.7385, 0.7605, 0.7595, 0.7672, 0.7563, 0.7573, 0.7597] |
|  Deep CNN (6 conv layers)   |         1263.86         |  [0.5273, 0.644, 0.7173, 0.7376, 0.7617, 0.7806, 0.79, 0.7934, 0.7866, 0.7999]   |
|       ResNet-like CNN       |         367.95          | [0.5765, 0.7297, 0.7712, 0.7977, 0.7942, 0.8295, 0.8112, 0.8175, 0.8213, 0.8415] |
+-----------------------------+-------------------------+----------------------------------------------------------------------------------+
```

![2.2_gradients.png](plots%2F2.2_gradients.png)

Среди всех 4 моделей наибольшую точность показала CNN с Residual связями. 
По диаграмме частотности значений градиентов видно, что модель с Residual связями имеет наиболее меньшие значения градиентов, что позволяет ей избежать эффектов "взрывающегося градиента". Однако такая модель может быть подвержена "исчезающему градиенту", что может привести к медленному обучению или полной остановке обучения нижних слоёв.

Feature map для CNN с Residual связями ([остальные](plots))
![2.1_ResNet-like CNN.png](plots%2F2.1_ResNet-like%20CNN.png)

## Задание 3: Кастомные слои и эксперименты (30 баллов)

Создайте файл `homework_custom_layers_experiments.py`:

### 3.1 Реализация кастомных слоев (15 баллов)
Реализуйте кастомные слои:
- [x] Кастомный сверточный слой с дополнительной логикой
- [x] Attention механизм для CNN
- [x] Кастомная функция активации
- [x] Кастомный pooling слой

```
# Для каждого слоя:
# - Реализуйте forward и backward проходы
# - Добавьте параметры если необходимо
# - Протестируйте на простых примерах
# - Сравните с стандартными аналогами
```

---

```commandline
Training Custom CNN...
Epoch 1/10: Train loss: 1.5650, Train acc: 0.4449, Test loss: 1.3166, Test acc: 0.5262,
Epoch 2/10: Train loss: 1.1778, Train acc: 0.5826, Test loss: 1.1135, Test acc: 0.6063,
Epoch 3/10: Train loss: 1.0196, Train acc: 0.6417, Test loss: 1.0410, Test acc: 0.6360,
Epoch 4/10: Train loss: 0.9303, Train acc: 0.6766, Test loss: 0.9685, Test acc: 0.6554,
Epoch 5/10: Train loss: 0.8622, Train acc: 0.6990, Test loss: 0.9549, Test acc: 0.6651,
Epoch 6/10: Train loss: 0.8117, Train acc: 0.7191, Test loss: 0.9153, Test acc: 0.6813,
Epoch 7/10: Train loss: 0.7662, Train acc: 0.7359, Test loss: 0.9295, Test acc: 0.6709,
Epoch 8/10: Train loss: 0.7303, Train acc: 0.7484, Test loss: 0.9022, Test acc: 0.6842,
Epoch 9/10: Train loss: 0.6975, Train acc: 0.7591, Test loss: 0.9114, Test acc: 0.6864,
Epoch 10/10: Train loss: 0.6679, Train acc: 0.7690, Test loss: 0.9467, Test acc: 0.6752,

Training Standard CNN...
Epoch 1/10: Train loss: 1.4347, Train acc: 0.4745, Test loss: 1.1172, Test acc: 0.6111,
Epoch 2/10: Train loss: 1.0381, Train acc: 0.6333, Test loss: 0.9327, Test acc: 0.6646,
Epoch 3/10: Train loss: 0.8580, Train acc: 0.6969, Test loss: 0.8696, Test acc: 0.6916,
Epoch 4/10: Train loss: 0.7404, Train acc: 0.7381, Test loss: 0.7679, Test acc: 0.7321,
Epoch 5/10: Train loss: 0.6478, Train acc: 0.7712, Test loss: 0.7371, Test acc: 0.7483,
Epoch 6/10: Train loss: 0.5697, Train acc: 0.7989, Test loss: 0.7337, Test acc: 0.7497,
Epoch 7/10: Train loss: 0.5008, Train acc: 0.8232, Test loss: 0.7443, Test acc: 0.7547,
Epoch 8/10: Train loss: 0.4398, Train acc: 0.8416, Test loss: 0.7812, Test acc: 0.7497,
Epoch 9/10: Train loss: 0.3869, Train acc: 0.8622, Test loss: 0.8031, Test acc: 0.7510,
Epoch 10/10: Train loss: 0.3371, Train acc: 0.8798, Test loss: 0.8016, Test acc: 0.7619,
```

Моя модель с кастомными слоями, Attention механизмом и кастомной функцией активации (Parametric Exponential Linear Unit) (все это без backward подхода) показала себя хуже, чем стандартная реализация сверточной сети.
Вероятно это связано с не самым удачным проектированием кастомных слоев и недостаточной настройкой параметров, благодаря чему стандартная модель ввиду своей простоты смогла отработать лучше на CIFAR-датасете, чем усложненная, но недостаточно сконфигурированная кастомная сеть.

### 3.2 Эксперименты с Residual блоками (15 баллов)

Исследуйте различные варианты Residual блоков:
- [x] Базовый Residual блок
- [x] Bottleneck Residual блок
- [x] Wide Residual блок

```
# Для каждого варианта:
# - Реализуйте блок с нуля
# - Сравните производительность
# - Проанализируйте количество параметров
# - Исследуйте стабильность обучения
```

---

```commandline
Training Baseline ResNet...
Epoch 1/10: Train loss: 1.3087, Train acc: 0.5241, Test loss: 1.3156, Test acc: 0.5343,
Epoch 2/10: Train loss: 0.8062, Train acc: 0.7156, Test loss: 0.7789, Test acc: 0.7329,
Epoch 3/10: Train loss: 0.5900, Train acc: 0.7940, Test loss: 0.6247, Test acc: 0.7834,
Epoch 4/10: Train loss: 0.4632, Train acc: 0.8382, Test loss: 0.5689, Test acc: 0.8017,
Epoch 5/10: Train loss: 0.3664, Train acc: 0.8725, Test loss: 0.5518, Test acc: 0.8206,
Epoch 6/10: Train loss: 0.2788, Train acc: 0.9026, Test loss: 0.5170, Test acc: 0.8352,
Epoch 7/10: Train loss: 0.1994, Train acc: 0.9294, Test loss: 0.5823, Test acc: 0.8326,
Epoch 8/10: Train loss: 0.1492, Train acc: 0.9472, Test loss: 0.7150, Test acc: 0.8073,
Epoch 9/10: Train loss: 0.1061, Train acc: 0.9629, Test loss: 0.6071, Test acc: 0.8368,
Epoch 10/10: Train loss: 0.0881, Train acc: 0.9687, Test loss: 0.6582, Test acc: 0.8444,
Training time: 359.79759550094604 s

Training Bottleneck ResNet...
Epoch 1/10: Train loss: 1.3952, Train acc: 0.4987, Test loss: 1.1620, Test acc: 0.5929,
Epoch 2/10: Train loss: 0.9504, Train acc: 0.6634, Test loss: 0.8750, Test acc: 0.6952,
Epoch 3/10: Train loss: 0.7640, Train acc: 0.7285, Test loss: 0.7898, Test acc: 0.7269,
Epoch 4/10: Train loss: 0.6346, Train acc: 0.7779, Test loss: 0.7298, Test acc: 0.7504,
Epoch 5/10: Train loss: 0.5323, Train acc: 0.8138, Test loss: 0.6441, Test acc: 0.7783,
Epoch 6/10: Train loss: 0.4423, Train acc: 0.8443, Test loss: 0.6972, Test acc: 0.7811,
Epoch 7/10: Train loss: 0.3680, Train acc: 0.8700, Test loss: 0.6970, Test acc: 0.7830,
Epoch 8/10: Train loss: 0.3005, Train acc: 0.8945, Test loss: 0.5977, Test acc: 0.8090,
Epoch 9/10: Train loss: 0.2465, Train acc: 0.9127, Test loss: 0.6130, Test acc: 0.8101,
Epoch 10/10: Train loss: 0.1900, Train acc: 0.9331, Test loss: 0.6582, Test acc: 0.8098,
Training time: 885.4323382377625 s

Training Wide ResNet...
Epoch 1/10: Train loss: 1.4129, Train acc: 0.4890, Test loss: 1.0922, Test acc: 0.6039,
Epoch 2/10: Train loss: 0.8778, Train acc: 0.6884, Test loss: 0.8954, Test acc: 0.6986,
Epoch 3/10: Train loss: 0.6461, Train acc: 0.7738, Test loss: 0.6567, Test acc: 0.7728,
Epoch 4/10: Train loss: 0.4944, Train acc: 0.8281, Test loss: 0.5696, Test acc: 0.8091,
Epoch 5/10: Train loss: 0.3823, Train acc: 0.8655, Test loss: 0.5278, Test acc: 0.8212,
Epoch 6/10: Train loss: 0.2873, Train acc: 0.8998, Test loss: 0.5652, Test acc: 0.8290,
Epoch 7/10: Train loss: 0.2129, Train acc: 0.9241, Test loss: 0.5212, Test acc: 0.8401,
Epoch 8/10: Train loss: 0.1495, Train acc: 0.9466, Test loss: 0.5915, Test acc: 0.8372,
Epoch 9/10: Train loss: 0.1104, Train acc: 0.9609, Test loss: 0.5877, Test acc: 0.8486,
Epoch 10/10: Train loss: 0.0827, Train acc: 0.9708, Test loss: 0.6382, Test acc: 0.8467,
Training time: 979.7608935832977 s
```

```commandline
Параметры:
- BaselineResNet: 11,173,962
- BottleneckResNet: 13,958,986
- WideResNet: 44,595,786
```

В целом, все модели доказали примерно одинаковый результат точности, однако обучение моделей с Bottleneck и Wide Residual блоками заняло значительно больше времени, не давая существенного прироста точности. 
Это вполне может быть связанно с ошибками архитектуры илл недостаточной конфигурацией данных блоков и модели или особенностями датасета, из-за чего данные блоки не могут раскрыть свой потенциал.

Хоть и модель с базовыми Residual блоками и показала хороший результат за сравнительно низкое время обучения, она же стала наименее стабильной моделью при обучении. 
Наиболее стабильной оказалась модель с WideResNet блоками.
Заметен также значительный прирост в количестве параметров для моделей с кастомными блоками, особенно в случае с WideResNet моделью.